[
  {
    "objectID": "Hot.html",
    "href": "Hot.html",
    "title": "Hot",
    "section": "",
    "text": "from the press\n\n\n\n\nData Curation Alone Can Stabilize In-context Learning \nPaper | Dataset | Tweet | 6-Jun 2023\nReemphasize - data is the moat. data quality matters. Recent example: Falcon is trained on cleaned data, and performs well, with low volume data setup.\n\n\nSpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression \nPaper | Dataset | Tweet | 6-Jun 2023\nLLMs can fit on your phones now.\n\n\nDeductive Verification of Chain-of-Thought Reasoning \nPaper | Dataset | Tweet | 6-Jun 2023\nCoT: Intermediate steps can also hallucinations. So, apply logic checks.\n\n\nScoNe: Benchmarking Negation Reasoning in Language Models With Fine-Tuning and In-Context Learning \nPaper | Dataset | Tweet | 6-Jun 2023\nCan LLMs handle negation. Easy, not!\n\n\nChatDB: Augmenting LLMs with Databases as Their Symbolic Memory \nPaper | Dataset | Tweet | 6-Jun 2023\nHave LLMs access symbolic memory for complex task planning and execution.\n\n\nEnabling Large Language Models to Generate Text with Citations \nPaper | Dataset | Tweet | 5-Jun 2023\nAsking an LLM to cite seems to be a way of reducing hallucinations, and also allows verification.\n\n\nOrca: Progressive Learning from Complex Explanation Traces of GPT-4 \nPaper | Dataset | Tweet | 5-Jun 2023\nTeach smaller LLM via a bigger LLMs. But watchful of pitfalls of imitation.\n\n\nLLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion \nPaper | Dataset | Tweet | 5-Jun 2023\nAnother co-op LLM idea.\n\n\nJust Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback \nPaper | Dataset | Tweet | 5-Jun 2023\nCalibration in NLP is hard! Now, just ask the LLM to self-report.\n\n\nReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing \nPaper | Dataset | Tweet | 5-Jun 2023\nUse LLMs as reviewers:\n\n\nReimagining Retrieval Augmented Language Models for Answering Queries \nPaper | Dataset | Tweet | 2-Jun 2023\nSemi-parametric architectures can be enhanced with views, a query analyzer/planner, and provenance to make a significantly more powerful system for question answering in terms of accuracy and efficiency, and potentially for other NLP tasks\n\n\nThe RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only \nPaper | Dataset | Tweet | 2-Jun 2023\nPublicly release an extract of 600 billion tokens from our RefinedWeb dataset, and 1.3/7.5B parameters language models trained on it. This clean dataset was considered important for Falcon’s performance.\n\n\nDecision-Oriented Dialogue for Human-AI Collaboration \nPaper | Code | Tweet | 2-Jun 2023\nagents + humans collab to solve hard everyday problems\n\n\nBlockwise Parallel Transformer for Long Context Large Models \nPaper | Dataset | Tweet | 2-Jun 2023\nTrain 7B models with over 130K or 13B models with over 64K context windows on just 8 A100 GPUs!\n\n\nBirth of a Transformer: A Memory Viewpoint \nPaper | Dataset | Blog | 2-Jun 2023\nTransfomers form backbones in LLMs. How do they work? Thinking fast and slow. Training data -&gt; Associative Memory. In-context learning -&gt; Induction Heads.\n\n\nLLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model \nPaper | Code | Tweet| 1-Jun 2023\nVia this Adaptor, LLM can perform open-ended multi-modal instructions by merely introducing 14M parameters over LLaMA.\n\n\nEditing Commonsense Knowledge in GPT \nPaper | Dataset | Blog | 2-Jun 2023\nOutperfm fine-tuning by model-editing based on feedback data.\n\n\nImproving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model \nPaper | Dataset | Blog | 2-June 2023\nApplications of RAG (Retrieval Augmented Generation) and evaluation against GPT-4. It is a good pattern to apply!\n\n\nSQL-PaLM: Improved Large Language ModelAdaptation for Text-to-SQL \nPaper | Dataset | Blog | 2-May 2023\nA tailed model to inteact with DBs. Another important LLM pattern.\n\n\n\n\n\nLet’s Verify Step by Step \nPaper | Dataset | Blog | 31-May 2023\nto solve math problems with CoT\n\n\nLikelihood-Based Diffusion Language Models \nPaper | Code | Tweet |1-June 2023\nDiffusion comes to LLMs.\n\n\nTree of Thoughts: Deliberate Problem Solving with Large Language Models \nPaper | Code | Blog | May 2023\nGeneralzie CoT and solve harder problems\n\n\nTraining Trajectories of Language Models Across Scales \nPaper | Code | Tweet |31-May 2023\nHow do LLMs learn during training? Touches aspects like double-descent, hallucination.\nPerplexity is a strong predictor of in-context learning performance.\n\n\nPlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning \nPaper | Code | Tweet |1-June 2023\nDisticll symbolic knowledge. Small LLMs beat large LLMs in counter-factual planning.\n\n\nDeliberate then Generate: Enhanced Prompting Framework for Text Generation \nPaper | Code | Tweet |1-June 2023\nAnother technique in prompt-engineering\n\n\nDirect Preference Optimization: Your Language Model is Secretly a Reward Model \nPaper | Code | Tweet | 30-May 2023\nDont need RL in training LLMs. Leads to quicker computation.\n\n\nImproving Factuality and Reasoning in Language Models through Multiagent Debate \nPaper | Code | Blog | 30-May 2023\nLLM Co-Pilots working with each other.\n\n\nModel Dementia: Generated Data Makes Models Forget \nPaper | Code | Blog | 30-May 2023\nWhat if all data we find on the internet is LLM generated?\n\n\nBigTrans: Augmenting Large Language Models with Multilingual Translation Capability over 100 Languages \nPaper | Code | Blog | 30-May 2023\nEnhance the language translaion capablities of LLMs\n\n\nBiomedGPT: BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks \nPaper | Code | Blog | 29-May 2023\nObject Detection, Captioning, NLI, VQA, Classification, Masked LMs"
  },
  {
    "objectID": "Hot.html#june",
    "href": "Hot.html#june",
    "title": "Hot",
    "section": "",
    "text": "Data Curation Alone Can Stabilize In-context Learning \nPaper | Dataset | Tweet | 6-Jun 2023\nReemphasize - data is the moat. data quality matters. Recent example: Falcon is trained on cleaned data, and performs well, with low volume data setup.\n\n\nSpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression \nPaper | Dataset | Tweet | 6-Jun 2023\nLLMs can fit on your phones now.\n\n\nDeductive Verification of Chain-of-Thought Reasoning \nPaper | Dataset | Tweet | 6-Jun 2023\nCoT: Intermediate steps can also hallucinations. So, apply logic checks.\n\n\nScoNe: Benchmarking Negation Reasoning in Language Models With Fine-Tuning and In-Context Learning \nPaper | Dataset | Tweet | 6-Jun 2023\nCan LLMs handle negation. Easy, not!\n\n\nChatDB: Augmenting LLMs with Databases as Their Symbolic Memory \nPaper | Dataset | Tweet | 6-Jun 2023\nHave LLMs access symbolic memory for complex task planning and execution.\n\n\nEnabling Large Language Models to Generate Text with Citations \nPaper | Dataset | Tweet | 5-Jun 2023\nAsking an LLM to cite seems to be a way of reducing hallucinations, and also allows verification.\n\n\nOrca: Progressive Learning from Complex Explanation Traces of GPT-4 \nPaper | Dataset | Tweet | 5-Jun 2023\nTeach smaller LLM via a bigger LLMs. But watchful of pitfalls of imitation.\n\n\nLLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion \nPaper | Dataset | Tweet | 5-Jun 2023\nAnother co-op LLM idea.\n\n\nJust Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback \nPaper | Dataset | Tweet | 5-Jun 2023\nCalibration in NLP is hard! Now, just ask the LLM to self-report.\n\n\nReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing \nPaper | Dataset | Tweet | 5-Jun 2023\nUse LLMs as reviewers:\n\n\nReimagining Retrieval Augmented Language Models for Answering Queries \nPaper | Dataset | Tweet | 2-Jun 2023\nSemi-parametric architectures can be enhanced with views, a query analyzer/planner, and provenance to make a significantly more powerful system for question answering in terms of accuracy and efficiency, and potentially for other NLP tasks\n\n\nThe RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only \nPaper | Dataset | Tweet | 2-Jun 2023\nPublicly release an extract of 600 billion tokens from our RefinedWeb dataset, and 1.3/7.5B parameters language models trained on it. This clean dataset was considered important for Falcon’s performance.\n\n\nDecision-Oriented Dialogue for Human-AI Collaboration \nPaper | Code | Tweet | 2-Jun 2023\nagents + humans collab to solve hard everyday problems\n\n\nBlockwise Parallel Transformer for Long Context Large Models \nPaper | Dataset | Tweet | 2-Jun 2023\nTrain 7B models with over 130K or 13B models with over 64K context windows on just 8 A100 GPUs!\n\n\nBirth of a Transformer: A Memory Viewpoint \nPaper | Dataset | Blog | 2-Jun 2023\nTransfomers form backbones in LLMs. How do they work? Thinking fast and slow. Training data -&gt; Associative Memory. In-context learning -&gt; Induction Heads.\n\n\nLLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model \nPaper | Code | Tweet| 1-Jun 2023\nVia this Adaptor, LLM can perform open-ended multi-modal instructions by merely introducing 14M parameters over LLaMA.\n\n\nEditing Commonsense Knowledge in GPT \nPaper | Dataset | Blog | 2-Jun 2023\nOutperfm fine-tuning by model-editing based on feedback data.\n\n\nImproving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model \nPaper | Dataset | Blog | 2-June 2023\nApplications of RAG (Retrieval Augmented Generation) and evaluation against GPT-4. It is a good pattern to apply!\n\n\nSQL-PaLM: Improved Large Language ModelAdaptation for Text-to-SQL \nPaper | Dataset | Blog | 2-May 2023\nA tailed model to inteact with DBs. Another important LLM pattern."
  },
  {
    "objectID": "Hot.html#may",
    "href": "Hot.html#may",
    "title": "Hot",
    "section": "",
    "text": "Let’s Verify Step by Step \nPaper | Dataset | Blog | 31-May 2023\nto solve math problems with CoT\n\n\nLikelihood-Based Diffusion Language Models \nPaper | Code | Tweet |1-June 2023\nDiffusion comes to LLMs.\n\n\nTree of Thoughts: Deliberate Problem Solving with Large Language Models \nPaper | Code | Blog | May 2023\nGeneralzie CoT and solve harder problems\n\n\nTraining Trajectories of Language Models Across Scales \nPaper | Code | Tweet |31-May 2023\nHow do LLMs learn during training? Touches aspects like double-descent, hallucination.\nPerplexity is a strong predictor of in-context learning performance.\n\n\nPlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning \nPaper | Code | Tweet |1-June 2023\nDisticll symbolic knowledge. Small LLMs beat large LLMs in counter-factual planning.\n\n\nDeliberate then Generate: Enhanced Prompting Framework for Text Generation \nPaper | Code | Tweet |1-June 2023\nAnother technique in prompt-engineering\n\n\nDirect Preference Optimization: Your Language Model is Secretly a Reward Model \nPaper | Code | Tweet | 30-May 2023\nDont need RL in training LLMs. Leads to quicker computation.\n\n\nImproving Factuality and Reasoning in Language Models through Multiagent Debate \nPaper | Code | Blog | 30-May 2023\nLLM Co-Pilots working with each other.\n\n\nModel Dementia: Generated Data Makes Models Forget \nPaper | Code | Blog | 30-May 2023\nWhat if all data we find on the internet is LLM generated?\n\n\nBigTrans: Augmenting Large Language Models with Multilingual Translation Capability over 100 Languages \nPaper | Code | Blog | 30-May 2023\nEnhance the language translaion capablities of LLMs\n\n\nBiomedGPT: BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks \nPaper | Code | Blog | 29-May 2023\nObject Detection, Captioning, NLI, VQA, Classification, Masked LMs"
  },
  {
    "objectID": "foreword.html",
    "href": "foreword.html",
    "title": "Foreword",
    "section": "",
    "text": "Foreword\nAlpan Raval\n\nWhat frame of mind is required to solve ML problems in applied settings?\n\nAdopt a problem-first mindset to a method-first mindset.\nInnovating on Algorithmic research does not pay off well as much as one expects, compared to investing on the problem, data, and overall solution to solve practical problems.\n\n\nFocus on Applied Research, not Algorithmic Research\n\nI hope that this edition of Effective ML practices helps adopt this mind to deliver value."
  },
  {
    "objectID": "papers/08_Survey.html",
    "href": "papers/08_Survey.html",
    "title": "Survey",
    "section": "",
    "text": "A retrospective look\n\n\n\n\nAn Overview on Language Models: Recent Developments and Outlook \nPaper | Code | Blog | Mar 2023\ncomment\n\n\nInteractive Natural Language Processing \nPaper | Code | Blog | May 2023\nLLMs as Agents. This seems to be a dominant pattern. This survey serves as an entry point for researchers who are interested in this rapidly evolving area and offers a broad view of the current landscape and future trajectory of iNLP.\n\n\n\n\n\nBridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation \nPaper | Code | Blog | May 2023\nHow to make models less toxic and how to leverage human feedback? What are the types of feedback?\n\n\n\n\n\nAugmented Language Models: a Survey \nPaper | Code | Blog | Feb 2023\nAn emerging design pattern - LLMs are controllers\n\n\n\n\n\nSurvey of Hallucination in Natural Language Generation \nPaper | Code | Blog | Nov 2022\nMetrics, Mitigation Methods and Future Direction! Should be a good read for everyone!\n\n\n\n\n\nRebooting AI in India \nPaper | Code | Blog | March 2023\nLandscape has changed since then. Training LLMs does not need billions, but data!"
  },
  {
    "objectID": "papers/08_Survey.html#foundations",
    "href": "papers/08_Survey.html#foundations",
    "title": "Survey",
    "section": "",
    "text": "An Overview on Language Models: Recent Developments and Outlook \nPaper | Code | Blog | Mar 2023\ncomment\n\n\nInteractive Natural Language Processing \nPaper | Code | Blog | May 2023\nLLMs as Agents. This seems to be a dominant pattern. This survey serves as an entry point for researchers who are interested in this rapidly evolving area and offers a broad view of the current landscape and future trajectory of iNLP."
  },
  {
    "objectID": "papers/08_Survey.html#alignment",
    "href": "papers/08_Survey.html#alignment",
    "title": "Survey",
    "section": "",
    "text": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation \nPaper | Code | Blog | May 2023\nHow to make models less toxic and how to leverage human feedback? What are the types of feedback?"
  },
  {
    "objectID": "papers/08_Survey.html#agency",
    "href": "papers/08_Survey.html#agency",
    "title": "Survey",
    "section": "",
    "text": "Augmented Language Models: a Survey \nPaper | Code | Blog | Feb 2023\nAn emerging design pattern - LLMs are controllers"
  },
  {
    "objectID": "papers/08_Survey.html#risk-safety",
    "href": "papers/08_Survey.html#risk-safety",
    "title": "Survey",
    "section": "",
    "text": "Survey of Hallucination in Natural Language Generation \nPaper | Code | Blog | Nov 2022\nMetrics, Mitigation Methods and Future Direction! Should be a good read for everyone!"
  },
  {
    "objectID": "papers/08_Survey.html#future",
    "href": "papers/08_Survey.html#future",
    "title": "Survey",
    "section": "",
    "text": "Rebooting AI in India \nPaper | Code | Blog | March 2023\nLandscape has changed since then. Training LLMs does not need billions, but data!"
  },
  {
    "objectID": "papers/index.html",
    "href": "papers/index.html",
    "title": "Papers",
    "section": "",
    "text": "Click through to any of these notebooks to get started DESIGN\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nApplications\n\n\n\n\n\n\n\nClassics\n\n\n\n\n\n\n\nDesign Patterns\n\n\n\n\n\n\n\nMethods\n\n\n\n\n\n\n\nSceptics\n\n\n\n\n\n\n\nSparks\n\n\n\n\n\n\n\nSurvey\n\n\n\n\n\n\n\nThe How & Why\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "papers/01_Classics.html",
    "href": "papers/01_Classics.html",
    "title": "Classics",
    "section": "",
    "text": "Classics\n\nSeminal works\n\n\nGPT-4 Technical Report \nPaper | Code | Blog | Mar 2023\nReports the development of GPT-4\n\n\nProximal Policy Optimization Algorithms \nPaper | Code | Blog | July 2017\nRL Policy used used in GPT-4’s RLHF.\n\n\nTraining language models to follow instructions with human feedback \nPaper | Code | Blog | Dec 2022\nKey in making generated output more responsible, humble, and realistic\n\n\nSelf-instruct: Aligning Language Models with Self Generated Instructions \nPaper | Code | Blog | Dec 2022\nInstruction data was critical for GPT’s performance and generalization. But these datasets are hard to come by. Can they be bootstrapped?\n\n\nLLaMA: Open and Efficient Foundation Language Models \nPaper | Code | Blog | Feb 2023\nThis work by Meta, paved way for many derivate models, as an alternative to GPT-4.\n\n\nLoRA: Low-Rank Adaptation of LLMs \nPaper | Code | Blog | Jun 2021\nVery useful in training and hosting LLMs on small datasets and low hardware settings. A key technique in fine-tuning LLMs in downstream tasks.\n\n\nQLoRA: Efficient Finetuning of Quantized LLMs \nPaper | Code | Blog | May 2023\nSimilar to LoRA but quantize the pre-trained LLMs.\n\n\nFlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness \nPaper | Code | Blog | Mar 2022\nGPU-optimized Attention.\n\n\nFast Transformer Decoding: One Write-Head is All You Need \nPaper | Code | Blog | Nov 2019\nMake training LLMs efficient."
  },
  {
    "objectID": "papers/04_Hope.html",
    "href": "papers/04_Hope.html",
    "title": "Sparks",
    "section": "",
    "text": "Sparks\n\nIs it the silver buller we are looking for?\n\n\nSparks of Artificial General Intelligence: Early experiments with GPT-4 \nPaper | Code | Blog | date\nFeels so. But is it for real?\n\n\nChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks \nPaper | Code | Blog | May 2023\nWow, beat them in thier own game!\n\n\nSPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning \nPaper | Code | Blog | May 2023\nWith Chain-of-Thought, LLMs can do well relative to RL in games."
  },
  {
    "objectID": "papers/05_Risks.html",
    "href": "papers/05_Risks.html",
    "title": "Sceptics",
    "section": "",
    "text": "Sceptics\n\ntoo good to be true?\n\n\nFrom Human-Centered to Social-Centered Artificial Intelligence: Assessing ChatGPT’s Impact through Disruptive Events \nPaper | Code | Blog | May 2023\nRisk profiling LLMs in terms of hallucinations - rely on a Human-centered, individualistic evaluation framework. Do we need to a societal view as well? How doe proliferation of chatGPT like models affect societies?\n\n\nThe False Promise of Imitating Proprietary LLMs \nPaper | Code | Blog | May 2023\nAre the public models trained on imitated data (generated by bigger models like GPT) any good? On indepedent datasets, the performance gaps are significant. Therefore, real data is the moat, not imitated/synthetic data!\n\n\nAre Emergent Abilities of Large Language Models a Mirage? \nPaper | Code | Blog | May 2023\nEmergent abilities are researcher’s choice of metrics, but not fundamental to LLMs. Need better evaluations."
  },
  {
    "objectID": "papers/02_How.html",
    "href": "papers/02_How.html",
    "title": "The How & Why",
    "section": "",
    "text": "Under the hood, why & how LLMs work?\n\n\n\n\nLanguage Models are Few-Shot Learners \nPaper | Code | Blog | May 2020\nDitch the paradigm of pre-train, fine-tune. Towards One model, many tasks.\n\n\n\n\n\nEliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions \nPaper | Dataset | Blog | Jun 2023\nEven when not explicitly trained language translation tasks, LLMs exhibit good zero shot ability.\n\n\nBirth of a Transformer: A Memory Viewpoint \nPaper | Dataset | Blog | 2-Jun 2023\nTransfomers form backbones in LLMs. How do they work? Thinking fast and slow. Training data -&gt; Associative Memory. In-context learning -&gt; Induction Heads.\n\n\nIntriguing Properties of Quantization at Scale \nPaper | Code | Tweet |31-May 2023\nAre quantization cliffs in performance solely a factor of scale? Apparently not. Quantize models ranging in size from 410M to 52B with minimal degradation in performance\n\n\nTraining Trajectories of Language Models Across Scales \nPaper | Code | Tweet |31-May 2023\nHow do LLMs learn during training? Touches aspects like double-descent, hallucination.\nPerplexity is a strong predictor of in-context learning performance.\n\n\n\n\n\nWhat Can Transformers Learn In-Context? A Case Study of Simple Function Classes \nPaper | Code | Blog | Aug 2022\nUsing Linear models as a probing device - shows what ICL does?\n\n\nWhat learning algorithm is in-context learning? Investigations with linear models \nPaper | Code | Blog | Nov 2022\nin-context learning recovers standard algorithms in linear model setup.\n\n\nWhy Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers \nPaper | Code | Blog | Dec 2022\nUses linear models as a lens to understand what in-context learnign does? It is a meta gradiant optimizer.\n\n\n\n\n\nScaling Data-Constrained Language Models \nPaper | Code | Blog | May 2023\nScale data and model size. How big is big? Can data be reused and selectively reused? Yes. Has someuseful datasets!\n\n\nLIMA: Less Is More for Alignment \nPaper | Code | Blog | May 2023\nA relief. Low volume but high quality instruction data is good. No RLHF required!!!\n\n\n\n\n\nHow Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection \nPaper | Code | Blog | date\nEasy jobs will be eiliminated. Higher order skills needed to survive.\n\n\nEvaluating ChatGPT’s Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness \nPaper | Code | Blog | May 2023\nFaithful and Explainable, but overconfident. GPTs works well in Open IE tasks but on Closed IE tasks, narrow, purpose models can be better.\n\n\n\n\n\nGPT in 60 Lines of NumPy \nJay Moody\nPaper | Code | Blog | YouTube | Tweet | Jun 2023\nFor educational purposes\n\n\nLet’s build GPT from scratch \nAndrej Karpathy\nPaper | Code | Blog | YouTube | Twitter | Jan 2023\nHmm, timely to steer out of ambiguity. Trade-off hype for reality. Performance, Latency, Cost - matter more than novelty, as always!"
  },
  {
    "objectID": "papers/02_How.html#perspectives",
    "href": "papers/02_How.html#perspectives",
    "title": "The How & Why",
    "section": "",
    "text": "Language Models are Few-Shot Learners \nPaper | Code | Blog | May 2020\nDitch the paradigm of pre-train, fine-tune. Towards One model, many tasks."
  },
  {
    "objectID": "papers/02_How.html#probes",
    "href": "papers/02_How.html#probes",
    "title": "The How & Why",
    "section": "",
    "text": "Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions \nPaper | Dataset | Blog | Jun 2023\nEven when not explicitly trained language translation tasks, LLMs exhibit good zero shot ability.\n\n\nBirth of a Transformer: A Memory Viewpoint \nPaper | Dataset | Blog | 2-Jun 2023\nTransfomers form backbones in LLMs. How do they work? Thinking fast and slow. Training data -&gt; Associative Memory. In-context learning -&gt; Induction Heads.\n\n\nIntriguing Properties of Quantization at Scale \nPaper | Code | Tweet |31-May 2023\nAre quantization cliffs in performance solely a factor of scale? Apparently not. Quantize models ranging in size from 410M to 52B with minimal degradation in performance\n\n\nTraining Trajectories of Language Models Across Scales \nPaper | Code | Tweet |31-May 2023\nHow do LLMs learn during training? Touches aspects like double-descent, hallucination.\nPerplexity is a strong predictor of in-context learning performance."
  },
  {
    "objectID": "papers/02_How.html#in-context-learning",
    "href": "papers/02_How.html#in-context-learning",
    "title": "The How & Why",
    "section": "",
    "text": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes \nPaper | Code | Blog | Aug 2022\nUsing Linear models as a probing device - shows what ICL does?\n\n\nWhat learning algorithm is in-context learning? Investigations with linear models \nPaper | Code | Blog | Nov 2022\nin-context learning recovers standard algorithms in linear model setup.\n\n\nWhy Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers \nPaper | Code | Blog | Dec 2022\nUses linear models as a lens to understand what in-context learnign does? It is a meta gradiant optimizer."
  },
  {
    "objectID": "papers/02_How.html#scaling-laws",
    "href": "papers/02_How.html#scaling-laws",
    "title": "The How & Why",
    "section": "",
    "text": "Scaling Data-Constrained Language Models \nPaper | Code | Blog | May 2023\nScale data and model size. How big is big? Can data be reused and selectively reused? Yes. Has someuseful datasets!\n\n\nLIMA: Less Is More for Alignment \nPaper | Code | Blog | May 2023\nA relief. Low volume but high quality instruction data is good. No RLHF required!!!"
  },
  {
    "objectID": "papers/02_How.html#evaluation",
    "href": "papers/02_How.html#evaluation",
    "title": "The How & Why",
    "section": "",
    "text": "How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection \nPaper | Code | Blog | date\nEasy jobs will be eiliminated. Higher order skills needed to survive.\n\n\nEvaluating ChatGPT’s Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness \nPaper | Code | Blog | May 2023\nFaithful and Explainable, but overconfident. GPTs works well in Open IE tasks but on Closed IE tasks, narrow, purpose models can be better."
  },
  {
    "objectID": "papers/02_How.html#by-code",
    "href": "papers/02_How.html#by-code",
    "title": "The How & Why",
    "section": "",
    "text": "GPT in 60 Lines of NumPy \nJay Moody\nPaper | Code | Blog | YouTube | Tweet | Jun 2023\nFor educational purposes\n\n\nLet’s build GPT from scratch \nAndrej Karpathy\nPaper | Code | Blog | YouTube | Twitter | Jan 2023\nHmm, timely to steer out of ambiguity. Trade-off hype for reality. Performance, Latency, Cost - matter more than novelty, as always!"
  },
  {
    "objectID": "papers/07_Methods.html",
    "href": "papers/07_Methods.html",
    "title": "Methods",
    "section": "",
    "text": "Methods and Techniques\n\n\n\n\nSpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression \nPaper | Dataset | Tweet | 6-Jun 2023\nLLMs can fit on your phones now.\n\n\nLLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model \nPaper | Code | Tweet| Jun 2023\nVia this Adaptor, LLM can perform open-ended multi-modal instructions by merely introducing 14M parameters over LLaMA.\n\n\nQLoRA: Efficient Finetuning of Quantized LLMs \nPaper | Code | Blog| May 2023\nQuantize a pre-trained model, add an adapter and fine-tune via LoRA: a reciepe for creating new LLMs\n\n\nDistilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes \nPaper | Code | Blog| May 2023\nAnother type of PEFT (Performance Efficient Fine Tuning) like LoRA family, but train smaller models on narrow tasks, with smaller datasets.\n\n\nSparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot \nPaper | Code | Blog| Jan 2023\nWithout any retraining, prune large models under few hours\n\n\nCramming: Training a Language Model on a Single GPU in One Day \nPaper | Code | Blog | Dec 2022\nCram It :)\n\n\n\n\n\nBlockwise Parallel Transformer for Long Context Large Models \nPaper | Dataset | Tweet | 2-Jun 2023\nTrain 7B models with over 130K or 13B models with over 64K context windows on just 8 A100 GPUs!\n\n\n\n\n\nSLiC-HF: Sequence Likelihood Calibration with Human Feedback \nPaper | Code | Blog| May 2023\nA simpler policy than PPO for use in RLHF, a key ingradient in GPT’s success\n\n\n\n\n\nIterative Forward Tuning Boosts In-context Learning in Language Models \nPaper | Code | Blog| May 2023\nAnother type of prompting? Like ICL, no-backprop but still adapt on examples!\n\n\nSmall Models are Valuable Plug-ins for Large Language Models \nPaper | Code | Blog| May 2023\nUse small models to provide the context. Big models will do better.\n\n\n\n\n\nMultimodal Chain-of-Thought Reasoning in Language Models \nPaper | Code | Blog | date\nReason with multiple modlas in the chain of thoughts!\n\n\n\n\n\nUnifying (Machine) Vision via Counterfactual World Modeling \nPaper | Code | Blog | Jun 2023\nLLMs can handle vision & speech\n\n\nPolyVoice: Language Models for Speech to Speech Translation \nPaper | Code | Tweet | Jun 2023\nLLMs can handle vision & speech"
  },
  {
    "objectID": "papers/07_Methods.html#peft",
    "href": "papers/07_Methods.html#peft",
    "title": "Methods",
    "section": "",
    "text": "SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression \nPaper | Dataset | Tweet | 6-Jun 2023\nLLMs can fit on your phones now.\n\n\nLLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model \nPaper | Code | Tweet| Jun 2023\nVia this Adaptor, LLM can perform open-ended multi-modal instructions by merely introducing 14M parameters over LLaMA.\n\n\nQLoRA: Efficient Finetuning of Quantized LLMs \nPaper | Code | Blog| May 2023\nQuantize a pre-trained model, add an adapter and fine-tune via LoRA: a reciepe for creating new LLMs\n\n\nDistilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes \nPaper | Code | Blog| May 2023\nAnother type of PEFT (Performance Efficient Fine Tuning) like LoRA family, but train smaller models on narrow tasks, with smaller datasets.\n\n\nSparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot \nPaper | Code | Blog| Jan 2023\nWithout any retraining, prune large models under few hours\n\n\nCramming: Training a Language Model on a Single GPU in One Day \nPaper | Code | Blog | Dec 2022\nCram It :)"
  },
  {
    "objectID": "papers/07_Methods.html#build-train",
    "href": "papers/07_Methods.html#build-train",
    "title": "Methods",
    "section": "",
    "text": "Blockwise Parallel Transformer for Long Context Large Models \nPaper | Dataset | Tweet | 2-Jun 2023\nTrain 7B models with over 130K or 13B models with over 64K context windows on just 8 A100 GPUs!"
  },
  {
    "objectID": "papers/07_Methods.html#rl",
    "href": "papers/07_Methods.html#rl",
    "title": "Methods",
    "section": "",
    "text": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback \nPaper | Code | Blog| May 2023\nA simpler policy than PPO for use in RLHF, a key ingradient in GPT’s success"
  },
  {
    "objectID": "papers/07_Methods.html#prompting",
    "href": "papers/07_Methods.html#prompting",
    "title": "Methods",
    "section": "",
    "text": "Iterative Forward Tuning Boosts In-context Learning in Language Models \nPaper | Code | Blog| May 2023\nAnother type of prompting? Like ICL, no-backprop but still adapt on examples!\n\n\nSmall Models are Valuable Plug-ins for Large Language Models \nPaper | Code | Blog| May 2023\nUse small models to provide the context. Big models will do better."
  },
  {
    "objectID": "papers/07_Methods.html#thinkers",
    "href": "papers/07_Methods.html#thinkers",
    "title": "Methods",
    "section": "",
    "text": "Multimodal Chain-of-Thought Reasoning in Language Models \nPaper | Code | Blog | date\nReason with multiple modlas in the chain of thoughts!"
  },
  {
    "objectID": "papers/07_Methods.html#beyond-words",
    "href": "papers/07_Methods.html#beyond-words",
    "title": "Methods",
    "section": "",
    "text": "Unifying (Machine) Vision via Counterfactual World Modeling \nPaper | Code | Blog | Jun 2023\nLLMs can handle vision & speech\n\n\nPolyVoice: Language Models for Speech to Speech Translation \nPaper | Code | Tweet | Jun 2023\nLLMs can handle vision & speech"
  },
  {
    "objectID": "papers/03_Apps.html",
    "href": "papers/03_Apps.html",
    "title": "Applications",
    "section": "",
    "text": "Applications\n\nWhat all can be used for?\n\n\nHarnessing the power of LLMs: A Survey on ChatGPT and Beyond \nPaper | Code | Blog | April 2023\nDiscuss the use and non-use of LLMs. In the accompaining blog, evolutionary tree of LLMs is provided.\n\n\n100 usecases of chatGPT \nPaper | Code | Blog | April 2023\nMany usecases – mostly for personal productivity!"
  },
  {
    "objectID": "papers/06_Patterns.html",
    "href": "papers/06_Patterns.html",
    "title": "Design Patterns",
    "section": "",
    "text": "Design Patterns\n\nHow should LLMs be used & applied?\n\n\nDeductive Verification of Chain-of-Thought Reasoning \nPaper | Dataset | Tweet | 6-Jun 2023\nCoT: Intermediate steps can also hallucinations. So, apply logic checks.\n\n\nScoNe: Benchmarking Negation Reasoning in Language Models With Fine-Tuning and In-Context Learning \nPaper | Dataset | Tweet | 6-Jun 2023\nCan LLMs handle negation. Easy, not!\n\n\nChatDB: Augmenting LLMs with Databases as Their Symbolic Memory \nPaper | Dataset | Tweet | 6-Jun 2023\nHave LLMs access symbolic memory for complex task planning and execution.\n\n\nEnabling Large Language Models to Generate Text with Citations \nPaper | Dataset | Tweet | 5-Jun 2023\nAsking an LLM to cite seems to be a way of reducing hallucinations, and also allows verification.\n\n\nLLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion \nPaper | Dataset | Tweet | 5-Jun 2023\nAnother co-op LLM idea.\n\n\nReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing \nPaper | Dataset | Tweet | 5-Jun 2023\nUse LLMs as reviewers:\n\n\nReimagining Retrieval Augmented Language Models for Answering Queries \nPaper | Dataset | Tweet | 2-Jun 2023\nSemi-parametric architectures can be enhanced with views, a query analyzer/planner, and provenance to make a significantly more powerful system for question answering in terms of accuracy and efficiency, and potentially for other NLP tasks\n\n\nDecision-Oriented Dialogue for Human-AI Collaboration \nPaper | Code | Tweet | 2-Jun 2023\nagents + humans collab to solve hard everyday problems\n\n\nImproving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model \nPaper | Dataset | Blog | 2-May 2023\nApplications of RAG (Retrieval Augmented Generation) and evaluation against GPT-4. It is a good pattern to apply!\n\n\nSQL-PaLM: Improved Large Language ModelAdaptation for Text-to-SQL \nPaper | Dataset | Blog | 2-May 2023\nA tailed model to inteact with DBs. Another important LLM pattern.\n\n\nLet’s Verify Step by Step \nPaper | Dataset | Blog | 31-May 2023\nto solve math problems with CoT\n\n\nTree of Thoughts: Deliberate Problem Solving with Large Language Models \nPaper | Code | Blog | May 2023\nGeneralzie CoT and solve harder problems\n\n\nInteractive Natural Language Processing \nPaper | Code | Blog | May 2023\nLLMs as Agents. This seems to be a dominant pattern. This survey serves as an entry point for researchers who are interested in this rapidly evolving area and offers a broad view of the current landscape and future trajectory of iNLP.\n\n\nReasoning with Language Model is Planning with World Model \nPaper | Code | Blog | May 2023\nLLMs as world models, reasoning & planning agents.\n\n\nHow Does Generative Retrieval Scale to Millions of Passages? \nPaper | Code | Blog | May 2023\nAn emerging pattern in LLMs is Retriveal Augmented Information Retrievel. Can Google search be made generative?\n\n\nSmall Models are Valuable Plug-ins for Large Language Models \nPaper | Code | Blog| May 2023\nUse small models to provide the context. Big models will do better.\n\n\nCrawling the Internal Knowledge-Base of Language Models \nPaper | Code | Blog| April 2023\nInteresting. Thefore, treat LLMs as indexed web, and create structured data from unstructred web.\n\n\nHuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face \nPaper | Code | Blog | Mar 2023\nLLMs as controllers.\n\n\nCollaborating with language models for embodied reasoning \nPaper | Code | Blog | Feb 2023\nA planner- actor- reporter: multiple LLMs work with each other.\n\n\nDescribe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents \nPaper | Code | Blog | Feb 2023\nBetter error corrections by breaking the task into chunks!\n\n\nPre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing \nPaper | Code | Blog | Jul 2021\nThe new way of solving NLP tasks: pre-train, prompt, and predict."
  },
  {
    "objectID": "blogs/01_top.html",
    "href": "blogs/01_top.html",
    "title": "Top",
    "section": "",
    "text": "insightful blogs and other resources\n\n\n\n\nWhat Is ChatGPT Doing … and Why Does It Work? \nStephen Wolfram\nPaper | Code | Blog | YouTube | Twitter | Feb 2023\nStrenghts & Limitations. Long read.\n\n\nThe Road to chatGPT \nRama Ramakrishnan\nPaper | Code | pdf | YouTube | Twitter | Mar 2023\nFor a layman - how chatGPT works. Nice & Quick.\n\n\nIllustrating Reinforcement Learning from Human Feedback (RLHF) \nHuggingFace\nPaper | Code | Blog | YouTube | Twitter | Dec 2022\nA nice tutorial on RLHF - a key piece in GPT family.\n\n\nUnderstanding Parameter-Efficient Finetuning of Large Language Models: From Prefix Tuning to LLaMA-Adapters \nSebastian Bubeck\nPaper | Code | Blog | YouTube | Twitter | Apr 2023\nAdapters - dock trainable network to a pre-trained LLM and fine-tune.\n\n\nWhy AI is incredibly smart and shockingly stupid \nYejn Choi\nPaper | Code | Blog | YouTube | TED | May 2023\nIs common sense important? Can AI be taught common sense. We need common-sense data.\n\n\nUnderstanding pre-trained LLMs through Probabilistic Lens \nXinyi Wang\nPaper | Code | pdf | YouTube | Twitter | Feb 2023\nSomewhat a math-oriented explaination.\n\n\nPrompt Engineering \nLillian Wang\nPaper | Code | Blog | YouTube | Twitter | May 2023\nI have a hammer. Give me a nail. Watchout for this to advance!\n\n\nHistory of LLaMA \nAndrew @ AGI Sphere\nPaper | Code | Blog | YouTube | Twitter | Apr 2023\nTracks the history of LLaMA and future derivates.\n\n\nExploring ChatGPT vs open-source models on slightly harder tasks \nMarco Tulio Ribeiro\nPaper | Code | Blog | YouTube | Twitter | May 2023\nWell. GPT-4 is good.\n\n\n\n\n\nIntegrating ChatGPT with internal knowledge base and question-answer platform \nQuy Tang\nPaper | Code | Blog | YouTube | Twitter | Mar 2023\nRetriveal Augmented Informatrion Extraction - an important design pattern\n\n\nLLMs as Recommender Systems \nSumit Kumar\nPaper | Code | Blog | YouTube | Twitter | May 2023\nI have a hammer. Give me a nail. Watchout for this to advance!\n\n\nUnifying LLM-powered QA Techniques with Routing Abstractions \nJerry Liu\nPaper | Code | Blog | YouTube | Twitter | May 2023\nRouter design pattern to search.\n\n\n\n\n\nState of GPT \nAndrej Karpathy \nPaper | Code | Blog | YouTube | Twitter | May 2023\nNice overview. Thinking Fast (Transformers & Tokens) and Thinking Slow (Chain of Thoughts).\n\n\nEmergence in Large Language Models \nJason Wei \nPaper | Code | Blog | YouTube | Tweet | May 2023\nNice overview. Thinking Fast (Transformers & Tokens) and Thinking Slow (Chain of Thoughts).\n\n\nReinforcement Learning from Human Feedback: Progress and Challenges \nJohn Schulman\nPaper | Code | Blog | YouTube | Twitter | Apr 2023\nRL in GPT is researchy (Dont try at home :) Dont try. Fine-tuning is OK.\n\n\nSparks of AGI \nSebastian Bubeck\nPaper | Code | Blog | YouTube | Twitter | Apr 2023\nStrenghts & Limitations\n\n\nAI Canon \nAnderseen Horowitz\nPaper | Code | Blog | YouTube | Twitter | May 2023\nYes, an AI Canon - of useful resources. This [what you are reading] blog is then a Howtizer :)\n\n\nBeyond Bias to Action: AI Startups and Big Tech \nPrateek Raj\nPaper | Code | Blog | YouTube | Twitter | May 2023\nDisrupt or be disrupted. What if the incumbents are creating products at breakneck speed. Microsoft’s Windows 11 - get it?\n\n\nThe New Generative AI Infra Stack \nCowboy Ventures\nPaper | Code | Blog | YouTube | Twitter | May 2023\nNew Infra is needed to ship Generative AI products.\n\n\nLarge Language Models and the Future of the ML Infrastructure Stack \nOuterBounds\nPaper | Code | Blog | YouTube | Twitter | Apr 2023\nHow LLMs will shape ML Infrastructure and vice-versa. Train Dolly with MetaFlow for under USD300.\n\n\nModern language models refute Chomsky’s approach to language \nSteven Piantadosi\nPaper | Code | Blog | YouTube | Twitter | Mar 2023\nA debate rages again - this time with strong empircal success of neural language models, without any innate understanding of language.\n\n\n\n\n\nGPT in 60 Lines of NumPy \nJay Moody\nPaper | Code | Blog | YouTube | Tweet | Jun 2023\nFor educational purposes\n\n\nLet’s build GPT from scratch \nAndrej Karpathy\nPaper | Code | Blog | YouTube | Twitter | Jan 2023\nHmm, timely to steer out of ambiguity. Trade-off hype for reality. Performance, Latency, Cost - matter more than novelty, as always!\n\n\nBuilding LLM applications for production \nChip Huyen\nPaper | Code | Blog | YouTube | Twitter | Apr 2023\nHmm, timely to steer out of ambiguity. Trade-off hype for reality. Performance, Latency, Cost - matter more than novelty, as always!\n\n\nHow we made Cerebras-GPT \nCerebras Systems\nPaper | Code | Blog | YouTube | Twitter | Q1 2023\nThe hardware tricks of the trade.\n\n\nFree Dolly: Introducing the World’s First Truly Open Instruction-Tuned LLM \nDataBricks\nPaper | Code | Blog | YouTube | Twitter | Apr 2023\nFor less USD30, train an LLM, whihc is commercially permissive, based off of GPT-Neo-X.\n\n\nHow to train your own LLMs \nreplit\nPaper | Code | Blog | YouTube | Twitter | May 2023\nLays out the steps.\n\n\nColossalChat: An Open-Source Solution for Cloning ChatGPT With a Complete RLHF Pipeline \nYang You\nPaper | Code | Blog | YouTube | Twitter | Mar 2023\nIncludes demo, training code, dataset, inference and model weights. Go create.\n\n\nThe Full Story of Large Language Models and RLHF \nAssembly AI\nPaper | Code | Blog | YouTube | Twitter | May 2023\nCreate LLMs - another take.\n\n\nFinetuning a commercially viable open source LLM (Flan-UL2) using Dolly15K and LoRA \nKevi Rohling\nPaper | Code | Blog | YouTube | Twitter | Apr 2023\nApply LoRA to create LLMS much quicker and cheaper.\n\n\nInstruction-tune models using your own data with txtinstruct \nDavid Mezzetti\nPaper | Code | Blog | YouTube | Twitter | May 2023\nIt works. Get model from HF and train."
  },
  {
    "objectID": "blogs/01_top.html#understanding",
    "href": "blogs/01_top.html#understanding",
    "title": "Top",
    "section": "",
    "text": "What Is ChatGPT Doing … and Why Does It Work? \nStephen Wolfram\nPaper | Code | Blog | YouTube | Twitter | Feb 2023\nStrenghts & Limitations. Long read.\n\n\nThe Road to chatGPT \nRama Ramakrishnan\nPaper | Code | pdf | YouTube | Twitter | Mar 2023\nFor a layman - how chatGPT works. Nice & Quick.\n\n\nIllustrating Reinforcement Learning from Human Feedback (RLHF) \nHuggingFace\nPaper | Code | Blog | YouTube | Twitter | Dec 2022\nA nice tutorial on RLHF - a key piece in GPT family.\n\n\nUnderstanding Parameter-Efficient Finetuning of Large Language Models: From Prefix Tuning to LLaMA-Adapters \nSebastian Bubeck\nPaper | Code | Blog | YouTube | Twitter | Apr 2023\nAdapters - dock trainable network to a pre-trained LLM and fine-tune.\n\n\nWhy AI is incredibly smart and shockingly stupid \nYejn Choi\nPaper | Code | Blog | YouTube | TED | May 2023\nIs common sense important? Can AI be taught common sense. We need common-sense data.\n\n\nUnderstanding pre-trained LLMs through Probabilistic Lens \nXinyi Wang\nPaper | Code | pdf | YouTube | Twitter | Feb 2023\nSomewhat a math-oriented explaination.\n\n\nPrompt Engineering \nLillian Wang\nPaper | Code | Blog | YouTube | Twitter | May 2023\nI have a hammer. Give me a nail. Watchout for this to advance!\n\n\nHistory of LLaMA \nAndrew @ AGI Sphere\nPaper | Code | Blog | YouTube | Twitter | Apr 2023\nTracks the history of LLaMA and future derivates.\n\n\nExploring ChatGPT vs open-source models on slightly harder tasks \nMarco Tulio Ribeiro\nPaper | Code | Blog | YouTube | Twitter | May 2023\nWell. GPT-4 is good."
  },
  {
    "objectID": "blogs/01_top.html#patterns",
    "href": "blogs/01_top.html#patterns",
    "title": "Top",
    "section": "",
    "text": "Integrating ChatGPT with internal knowledge base and question-answer platform \nQuy Tang\nPaper | Code | Blog | YouTube | Twitter | Mar 2023\nRetriveal Augmented Informatrion Extraction - an important design pattern\n\n\nLLMs as Recommender Systems \nSumit Kumar\nPaper | Code | Blog | YouTube | Twitter | May 2023\nI have a hammer. Give me a nail. Watchout for this to advance!\n\n\nUnifying LLM-powered QA Techniques with Routing Abstractions \nJerry Liu\nPaper | Code | Blog | YouTube | Twitter | May 2023\nRouter design pattern to search."
  },
  {
    "objectID": "blogs/01_top.html#perspectives",
    "href": "blogs/01_top.html#perspectives",
    "title": "Top",
    "section": "",
    "text": "State of GPT \nAndrej Karpathy \nPaper | Code | Blog | YouTube | Twitter | May 2023\nNice overview. Thinking Fast (Transformers & Tokens) and Thinking Slow (Chain of Thoughts).\n\n\nEmergence in Large Language Models \nJason Wei \nPaper | Code | Blog | YouTube | Tweet | May 2023\nNice overview. Thinking Fast (Transformers & Tokens) and Thinking Slow (Chain of Thoughts).\n\n\nReinforcement Learning from Human Feedback: Progress and Challenges \nJohn Schulman\nPaper | Code | Blog | YouTube | Twitter | Apr 2023\nRL in GPT is researchy (Dont try at home :) Dont try. Fine-tuning is OK.\n\n\nSparks of AGI \nSebastian Bubeck\nPaper | Code | Blog | YouTube | Twitter | Apr 2023\nStrenghts & Limitations\n\n\nAI Canon \nAnderseen Horowitz\nPaper | Code | Blog | YouTube | Twitter | May 2023\nYes, an AI Canon - of useful resources. This [what you are reading] blog is then a Howtizer :)\n\n\nBeyond Bias to Action: AI Startups and Big Tech \nPrateek Raj\nPaper | Code | Blog | YouTube | Twitter | May 2023\nDisrupt or be disrupted. What if the incumbents are creating products at breakneck speed. Microsoft’s Windows 11 - get it?\n\n\nThe New Generative AI Infra Stack \nCowboy Ventures\nPaper | Code | Blog | YouTube | Twitter | May 2023\nNew Infra is needed to ship Generative AI products.\n\n\nLarge Language Models and the Future of the ML Infrastructure Stack \nOuterBounds\nPaper | Code | Blog | YouTube | Twitter | Apr 2023\nHow LLMs will shape ML Infrastructure and vice-versa. Train Dolly with MetaFlow for under USD300.\n\n\nModern language models refute Chomsky’s approach to language \nSteven Piantadosi\nPaper | Code | Blog | YouTube | Twitter | Mar 2023\nA debate rages again - this time with strong empircal success of neural language models, without any innate understanding of language."
  },
  {
    "objectID": "blogs/01_top.html#build",
    "href": "blogs/01_top.html#build",
    "title": "Top",
    "section": "",
    "text": "GPT in 60 Lines of NumPy \nJay Moody\nPaper | Code | Blog | YouTube | Tweet | Jun 2023\nFor educational purposes\n\n\nLet’s build GPT from scratch \nAndrej Karpathy\nPaper | Code | Blog | YouTube | Twitter | Jan 2023\nHmm, timely to steer out of ambiguity. Trade-off hype for reality. Performance, Latency, Cost - matter more than novelty, as always!\n\n\nBuilding LLM applications for production \nChip Huyen\nPaper | Code | Blog | YouTube | Twitter | Apr 2023\nHmm, timely to steer out of ambiguity. Trade-off hype for reality. Performance, Latency, Cost - matter more than novelty, as always!\n\n\nHow we made Cerebras-GPT \nCerebras Systems\nPaper | Code | Blog | YouTube | Twitter | Q1 2023\nThe hardware tricks of the trade.\n\n\nFree Dolly: Introducing the World’s First Truly Open Instruction-Tuned LLM \nDataBricks\nPaper | Code | Blog | YouTube | Twitter | Apr 2023\nFor less USD30, train an LLM, whihc is commercially permissive, based off of GPT-Neo-X.\n\n\nHow to train your own LLMs \nreplit\nPaper | Code | Blog | YouTube | Twitter | May 2023\nLays out the steps.\n\n\nColossalChat: An Open-Source Solution for Cloning ChatGPT With a Complete RLHF Pipeline \nYang You\nPaper | Code | Blog | YouTube | Twitter | Mar 2023\nIncludes demo, training code, dataset, inference and model weights. Go create.\n\n\nThe Full Story of Large Language Models and RLHF \nAssembly AI\nPaper | Code | Blog | YouTube | Twitter | May 2023\nCreate LLMs - another take.\n\n\nFinetuning a commercially viable open source LLM (Flan-UL2) using Dolly15K and LoRA \nKevi Rohling\nPaper | Code | Blog | YouTube | Twitter | Apr 2023\nApply LoRA to create LLMS much quicker and cheaper.\n\n\nInstruction-tune models using your own data with txtinstruct \nDavid Mezzetti\nPaper | Code | Blog | YouTube | Twitter | May 2023\nIt works. Get model from HF and train."
  },
  {
    "objectID": "blogs/02_chronicles.html",
    "href": "blogs/02_chronicles.html",
    "title": "Chronicles",
    "section": "",
    "text": "Chronicles\n\nperspectives as they occur\n\n\nEvaluating and uncovering open LLMs \nNathan Lambert\nPaper | Code | Blog | YouTube | Twitter | May 2023\n\n\n\nAI is Eating The World \nThe Gradient\nPaper | Code | Blog | YouTube | Twitter | May 2023\nGives a view into the architecture and building blocks needed.\n\n\nVoyager: LLMs play games better than RL \nThe Gradient\nPaper | Code | Blog | YouTube | Twitter | May 2023\n\n\n\nModern AI is Domestification \nThe Gradient\nPaper | Code | Blog | YouTube | Twitter | May 2023\nWhat is your moat. Is your prior (on data) same as the LLMs - do prompting. If not fine-tune. You have no compute. Go for PEFT. If not fine tune. SFT on Imitation data might lead to false superriority as this Paper reported. Figure out your priors first. Strategy follows."
  },
  {
    "objectID": "blogs/index.html",
    "href": "blogs/index.html",
    "title": "Develop",
    "section": "",
    "text": "Click through to any of these notebooks to get started on DEVELOP\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nChronicles\n\n\n\n\n\n\n\nTop\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "datasets/index.html",
    "href": "datasets/index.html",
    "title": "Develop",
    "section": "",
    "text": "Click through to any of these notebooks to get started on DEVELOP\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nDatasets\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "datasets/01_data.html",
    "href": "datasets/01_data.html",
    "title": "Datasets",
    "section": "",
    "text": "Datasets\n\nTHE moat!\n\n\nGPT-4 Technical Report \nPaper | Code | Blog | Mar 2023\nReports the development of GPT-4\n\n\nProximal Policy Optimization Algorithms \nPaper | Code | Blog | July 2017\nRL Policy used used in GPT-4’s RLHF.\n\n\nTraining language models to follow instructions with human feedback \nPaper | Code | Blog | Dec 2022\nKey in making generated output more responsible, humble, and realistic\n\n\nSelf-instruct: Aligning Language Models with Self Generated Instructions \nPaper | Code | Blog | Dec 2022\nInstruction data was critical for GPT’s performance and generalization. But these datasets are hard to come by. Can they be bootstrapped?\n\n\nLLaMA: Open and Efficient Foundation Language Models \nPaper | Code | Blog | Feb 2023\nThis work by Meta, paved way for many derivate models, as an alternative to GPT-4.\n\n\nLoRA: Low-Rank Adaptation of LLMs \nPaper | Code | Blog | Jun 2021\nVery useful in training and hosting LLMs on small datasets and low hardware settings. A key technique in fine-tuning LLMs in downstream tasks."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LLM-Wiki",
    "section": "",
    "text": "LLM-Wiki\nNavigating the LLMs zoo!\n\n Fig. from here\nEversince chatGPT happenned - there is exicement and anxiety at the same time. AI community is advancing the science, engineering and know-how at breakneck speed. New advancements, breakthroughs, happenning by the hour! So much so that, it is becoming impossible to keep track. Here is “an” attempt, not the only one, to organize content for refernece.\nIn this wiki, we will curate a list of papers, blogs, code repositories, books, datasets that delve into the\n\ninner workings of LLMs, tracing their development from their humble beginnings to the state-of-the-art\ntheir architecture, training methodologies\nthe vast datasets that fuel their learning\nUnderstanding the mechanics behind these models will provide valuable insights into their capabilities and limitations\nreal-world applications of LLMs across diverse domains.\npotential benefits and risks associated with LLM deployment\nkey challenges and controversies surrounding LLMs. We will explore topics such as bias and fairness, data privacy, and the ethical responsibilities that accompany the development and use of these models.\nFuture of LLMs and the potential they hold for advancing human-machine collaboration.\n\nAs we navigate the uncharted territories of AI, it is crucial to anticipate the opportunities and challenges that lie ahead. The convergence of human intelligence and machine learning is a transformative force, and understanding its implications will empower us to shape a future that is beneficial for all.\nThis wiki aims to be a comprehensive guide, accessible to both the curious novice and the seasoned expert in the field. It offers a balanced perspective, presenting the complexities and nuances of LLMs in a clear and engaging manner. By the end of this journey, we hope you will have gained a deeper understanding of LLMs, their significance, and their impact on our world.\nIt is with great pleasure that we invite you to embark on this exciting voyage into the realm of LLMs. As you turn the pages, immerse yourself in the wonders and challenges of these remarkable language models. May this book serve as a catalyst for thought, discussion, and action, ultimately guiding us towards a future where human intelligence and artificial intelligence coexist harmoniously. [Note: This paragraph is generated by GPT-4]\nEnjoy the journey.\nSoma S Dhavala\nNote: All the views are mine, and does not reflect that of any individual or organization."
  },
  {
    "objectID": "codes/index.html",
    "href": "codes/index.html",
    "title": "Develop",
    "section": "",
    "text": "Click through to any of these notebooks to get started on DEVELOP\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nDemos\n\n\n\n\n\n\n\nFrameworks\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "codes/01_frameworks.html",
    "href": "codes/01_frameworks.html",
    "title": "Frameworks",
    "section": "",
    "text": "for the handyman!\n\n\n\n\nLangChain \ndoc | repo |\nBuild LLM applications and implement several patterns like Agents, Controllers, Retrieval-Augmented Information Retrieval/\nIntegrates with many LLMs and databases. Noe4j is a new addition.\n\n\nllama-index \ndoc | repo |\nUseful in connecting disperate datasources via LLMs\n\n\nChain Lit \ndoc | repo |\nBuild ChatGPT like UI. Integrated with LangChain.\n\n\nTxtAi \ndoc | repo |\nSearch with LLMs\n\n\nNeMo Guardrails \ndoc| repo |\nAdd programmable guardrails to LLMs\n\n\nLangChain AI PlugIN \ndoc | repo |\nLangChain itself as a PlugIn to LLMs.\n\n\nSemantic Kernel \ndoc | repo |\nIntegrate LLMs with regular programming languages.\n\n\nMiniChain \ndoc | repo |\nLLMs as Python functions.\n\n\n\n\n\nEasyLM \ndoc | repo |\npre-train, fine-tune LLMs like LLaMA, GPT-J and RoBERTa. Koala, OpenLLaMA are trained wth this.\n\n\nTxtInstruct \ndoc | repo |\nDatasets and Models for Instruction Tuning. See this example.\n\n\nDynosuar: Data Curation Framework \ndoc | code\nUse other LLMs to generate data.\n\n\nTool LLaMA \npaper | dataset | Code | Tweet\nCreate high quality data to train LLMs\n\n\npeft\ndoc | code\nLibrary for parameter efficient fine tuning. Implements LoRA, P-Tuning, AdaLoRA etc..\n\n\nQLoRA\npaper | code\nFine-tune quantized LLMs\n\n\nLLaMA Adapter\ndoc | code\nFine LLaMA models.\n\n\nDeepSpeed\ndoc | code\nA mature library from Microsoft to accelerate training Deep Learning models.\n\n\nLamini\ndoc | code\nA nice abstraction to train models in the cloud.\n\n\nminiGPT\ndoc | code)\nA 90 line code to understand how to code and train a LM.\n\n\ncFormer\ndoc | code\nImplementation in C/C++ to run LLMs on Laptops!\n\n\n\n\n\nLanarky\ndoc | code\nLanarky is an open-source framework to deploy LLM applications in production. It is built on top of FastAPI and comes with batteries included like LangChain"
  },
  {
    "objectID": "codes/01_frameworks.html#utils",
    "href": "codes/01_frameworks.html#utils",
    "title": "Frameworks",
    "section": "",
    "text": "LangChain \ndoc | repo |\nBuild LLM applications and implement several patterns like Agents, Controllers, Retrieval-Augmented Information Retrieval/\nIntegrates with many LLMs and databases. Noe4j is a new addition.\n\n\nllama-index \ndoc | repo |\nUseful in connecting disperate datasources via LLMs\n\n\nChain Lit \ndoc | repo |\nBuild ChatGPT like UI. Integrated with LangChain.\n\n\nTxtAi \ndoc | repo |\nSearch with LLMs\n\n\nNeMo Guardrails \ndoc| repo |\nAdd programmable guardrails to LLMs\n\n\nLangChain AI PlugIN \ndoc | repo |\nLangChain itself as a PlugIn to LLMs.\n\n\nSemantic Kernel \ndoc | repo |\nIntegrate LLMs with regular programming languages.\n\n\nMiniChain \ndoc | repo |\nLLMs as Python functions."
  },
  {
    "objectID": "codes/01_frameworks.html#train",
    "href": "codes/01_frameworks.html#train",
    "title": "Frameworks",
    "section": "",
    "text": "EasyLM \ndoc | repo |\npre-train, fine-tune LLMs like LLaMA, GPT-J and RoBERTa. Koala, OpenLLaMA are trained wth this.\n\n\nTxtInstruct \ndoc | repo |\nDatasets and Models for Instruction Tuning. See this example.\n\n\nDynosuar: Data Curation Framework \ndoc | code\nUse other LLMs to generate data.\n\n\nTool LLaMA \npaper | dataset | Code | Tweet\nCreate high quality data to train LLMs\n\n\npeft\ndoc | code\nLibrary for parameter efficient fine tuning. Implements LoRA, P-Tuning, AdaLoRA etc..\n\n\nQLoRA\npaper | code\nFine-tune quantized LLMs\n\n\nLLaMA Adapter\ndoc | code\nFine LLaMA models.\n\n\nDeepSpeed\ndoc | code\nA mature library from Microsoft to accelerate training Deep Learning models.\n\n\nLamini\ndoc | code\nA nice abstraction to train models in the cloud.\n\n\nminiGPT\ndoc | code)\nA 90 line code to understand how to code and train a LM.\n\n\ncFormer\ndoc | code\nImplementation in C/C++ to run LLMs on Laptops!"
  },
  {
    "objectID": "codes/01_frameworks.html#deploy",
    "href": "codes/01_frameworks.html#deploy",
    "title": "Frameworks",
    "section": "",
    "text": "Lanarky\ndoc | code\nLanarky is an open-source framework to deploy LLM applications in production. It is built on top of FastAPI and comes with batteries included like LangChain"
  },
  {
    "objectID": "codes/02_demos.html",
    "href": "codes/02_demos.html",
    "title": "Demos",
    "section": "",
    "text": "Demos\n\nbe inspired!\n\n\nAsk Anything - A Video Chat \nDoc | Code\nChat with a Video, not with text data.\n\n\nAuto GPT \nDoc | Code\nChain together multiple LLMs together."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nEffective ML: from Concepts to Actions\n\nEffective ML brings togeher\n\nscientific methods\nengineering tools\nand design thinking\n\nto solve practical problems.\n\nEffective ML is a set of practices and reocmmendations to build Performant, Observable, Reliable, Agile, Scalable, Faster, Cost-efficient Machine Learning\n\nThe question is – is it something new? Absolutely not. For an organization like Wadhwani AI, which focusses on applying ML/AI to solve problems, one has to counter the popular narrative, which overemphasies models over modeling. Here, modeling is all an all encompassing action verb.\nTherefore, in this edition of Effective ML, we break down the workflow into actions, and talk about which scinetific methods are useful, what engineering tools are available, and how to orchstrate these actions to build well designed, well engineered ML systems. Since models and the allied baggage is abundenty available within reach, we deliberately do not cover these aspect (i.e no algorithms and model dicussion).\nSpecifally, all topics dicussed will cover one of the following Actions:\n\nPrepare:\n\nCan we get ready to solve a problem even with zero problem-specific data? Enter the world of pre-trained models, public datasets and self-supervised learning.\nCan we be ready with the most obvious but dreaded question your clients would be asking - How much data do you need?\n\nDesign\n\nML is a piece of technology, maybe a glorified one. It is one lego piece in the puzzle! What is the puzzle we need to solve\nWhat are those lego pieces? How to solve a problem with ML and design such systems?\n\nWrite\n\nHow to write clean, readable, maintainable, modular, extensible code?\n\nDevelop\n\nHow can you collaborate with other team members? How do you manage Compute resources? Do you need to version data, code, runtimes yourself? How do you ensure your ML project is reproducible?\n\nDocument\n\nHow do you communicate the logic, pipelines, results, and metrics? Do you see writing code and documenting as separate activities? As results get updated, how do you keep your documentation up-to-date?\n\nDeploy\n\nA most important aspect of applied research is to deploy a solution. What are the many possible ways to deploy models? What are CI/CD tools to ensure only tested models get deployed?\n\nObserve\n\nWhat you can not observe, you can not understand. What you can not understand, you can not control! Observibility of an ML system pre- or post- deployment is a prerequisite to developing a healthy, verifiable ML system. How can you make your ML system - observable?\n\nTest\n\nThe IT industry, over a period of decades, developed a variety of tools, processes and practices to test software, to the extent that, new paradigms such as Test Driven Design, QA, Agile Methods, emerged. ML is nascent compared to IT & Software development – top that with inherent lack of determinism in ML systems. How to test systems, at various stages of development and deployment, in the presence of certain uncertainty.\n\nDiagnose\n\nTest & Train errors and some plots of accuracy vs time. Is that all we got? How to debug models, at various levels of granularity and from various angles?\n\nImprove/ iterate\n\nML systems, like Software products, are unfinished by design. Model development is an interactive exercise. What does it take to have a Continuous Improvement pipeline in-place?\n\n\nThe collection of works in this executable playbook is a result of 20+ people spending 20 hours together, spread across two days, listening, presenting and discussing 25+ topics. There were 15 presentations made by the ML team alone on a range of topics."
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "THE moat!\n\n\nDatasets Hub \ndatasets |\nTagged by Task like CoT, SFT, PT,…\n\n\n\n\nMultiLegalPile: A 689GB Multilingual Legal Corpus \nPaper | Dataset | Code | Tweet | 2-Jun 2023\nLegal Data - LegalGPT :P\n\n\nFalcon Data \nmodels | dataset | Code | Tweet |\n5k+parquet files, 960m+samples, 500-600 tokens\n\n\nThe RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only \nPaper | Dataset | Tweet | 2-Jun 2023\nPublicly release an extract of 600 billion tokens from our RefinedWeb dataset, and 1.3/7.5B parameters language models trained on it. This clean dataset was considered important for Falcon’s performance.\n\n\nThe Pile \npaper | dataset | Code | Blog |\nAn 825GB set consisting of 22 smaller, high quality datasets to train LLMs.  It is THE dataset from eleuther\n\n\nused in Alpaca \npaper | dataset | Code | Blog |\n52k instruction following data used in fine-tuning Alpaca model. This data was produced using GPT-3. So beware of the false superiority of the imitattion models.\n\n\nused in gpt4all, gpt4all-lora \ngpt4all | dataset | Code | Blog |\nAccessible from HuggingFace datasets library.\n\n\nThe OIG dataset \npaper | dataset | Code | Blog |\n43M Instruction dataset to convert a pre-trained LLM to follow instructions.\n\n\nfrom Anthropic \npaper | dataset | Code | Blog |\nHuman preference data to reduce model toxicity. Accessible from HuggingFace datasets library.\n\n\nfrom openAI \npaper | dataset | Code | Blog |\nA long-form QA to align model with humans. Accessible from HuggingFace datasets library.\n\n\nfrom openAI \npaper | dataset | Code | Blog |\ntrain a reward model, which helps an LLM align with Humans. Accessible from HuggingFace datasets library.\n\n\nStanford Human Preferences Dataset \npaper | dataset | Code | Blog |\nModels like Stable Vicuna are trained on this.\n\n\n\n\n\nDynosuar: Data Curation Framework \npaper | dataset | Code | Blog\nUse other LLMs to generate data.\n\n\nTool LLaMA \npaper | dataset | Code | Tweet\nCreate high quality data to train LLMs\n\n\nTxtInstruct \ndoc | repo |\nDatasets and Models for Instruction Tuning. See this example."
  },
  {
    "objectID": "datasets.html#datasets-1",
    "href": "datasets.html#datasets-1",
    "title": "Datasets",
    "section": "",
    "text": "MultiLegalPile: A 689GB Multilingual Legal Corpus \nPaper | Dataset | Code | Tweet | 2-Jun 2023\nLegal Data - LegalGPT :P\n\n\nFalcon Data \nmodels | dataset | Code | Tweet |\n5k+parquet files, 960m+samples, 500-600 tokens\n\n\nThe RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only \nPaper | Dataset | Tweet | 2-Jun 2023\nPublicly release an extract of 600 billion tokens from our RefinedWeb dataset, and 1.3/7.5B parameters language models trained on it. This clean dataset was considered important for Falcon’s performance.\n\n\nThe Pile \npaper | dataset | Code | Blog |\nAn 825GB set consisting of 22 smaller, high quality datasets to train LLMs.  It is THE dataset from eleuther\n\n\nused in Alpaca \npaper | dataset | Code | Blog |\n52k instruction following data used in fine-tuning Alpaca model. This data was produced using GPT-3. So beware of the false superiority of the imitattion models.\n\n\nused in gpt4all, gpt4all-lora \ngpt4all | dataset | Code | Blog |\nAccessible from HuggingFace datasets library.\n\n\nThe OIG dataset \npaper | dataset | Code | Blog |\n43M Instruction dataset to convert a pre-trained LLM to follow instructions.\n\n\nfrom Anthropic \npaper | dataset | Code | Blog |\nHuman preference data to reduce model toxicity. Accessible from HuggingFace datasets library.\n\n\nfrom openAI \npaper | dataset | Code | Blog |\nA long-form QA to align model with humans. Accessible from HuggingFace datasets library.\n\n\nfrom openAI \npaper | dataset | Code | Blog |\ntrain a reward model, which helps an LLM align with Humans. Accessible from HuggingFace datasets library.\n\n\nStanford Human Preferences Dataset \npaper | dataset | Code | Blog |\nModels like Stable Vicuna are trained on this."
  },
  {
    "objectID": "datasets.html#synthetic-data-creation-tools",
    "href": "datasets.html#synthetic-data-creation-tools",
    "title": "Datasets",
    "section": "",
    "text": "Dynosuar: Data Curation Framework \npaper | dataset | Code | Blog\nUse other LLMs to generate data.\n\n\nTool LLaMA \npaper | dataset | Code | Tweet\nCreate high quality data to train LLMs\n\n\nTxtInstruct \ndoc | repo |\nDatasets and Models for Instruction Tuning. See this example."
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "Preface\nSoma Dhavala"
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "Models",
    "section": "",
    "text": "THE obsession!\n\n\n\nChatBot Arena. Elo rating for models (public and private)\nLLM Collection. Collection of many LLMs in one place - check out other sections as well on prompt engineering.\n\n\n\nModels with Permissive Licenses\n\nOpen LLaMA \nmodels | paper | dataset | code | blog |\nAn Open Source reproduction of LLaMA for permissive commerical uses.\n\n\nFalcon \nmodels | paper| dataset | Code | Blog |\nNot based on LLaMA. Can use for commerical purposes. Now finetune.\n\n\nGPT-Neo-X \nmodels | paper | dataset | code | blog |\nMany in this small family at Pythia. Based on The Pile datset.\n\n\nGPT4All, GPT4ALL-LoRA \nmodels | paper | dataset | Code | Blog |\n\n\n\nOpen Assistant \nmodels | paper | dataset| Code | Blog |\n\n\n\nDolly 2 \nmodels | paper | dataset| Code | Blog |\nBased on GPT-Neo-X.\n\n\nBLOOM \nmodels | paper | dataset| Code | Blog |\n\n\n\nCerebras \nmodels | paper | dataset| Code | Blog |\n\n\n\n\n\nMany in this category are, for most part, are derivates of LLaMA. Since LLaMA model weights were never released officially, the following only publish the weight deltas. Thefore, overall model is not permissive for commerical usages.\n\nGuanaco \nmodels | paper| dataset| Code | Blog |\nFine-tune LLaMA on a single GPU via QLoRA.\n\n\nGorilla \nmodels | paper | dataset| Code | Blog |\nBeats GPT-4 on writing API calls.\n\n\nGoat \nmodels | paper | dataset| Code | Blog |\nBeats GPT-4 on Arithmetic tasks. Based on Alpaca-LoRA.\n\n\nStableVicuna \nmodels | paper | dataset| Code | Blog |\n\n\n\nVicuna \nmodels | paper | dataset| Code | Blog |\n\n\n\nKaola \nmodels | paper | dataset| Code | Blog |\nIntroduces EasyLM for code structuring and training in JAX.\n\n\nAlpaca \nmodels | Paper | dataset| code | Blog |\nAlpaca-LoRA can run Raspberry Pi\n\n\nLLaMA \nmodels | Paper | dataset| Blog |"
  },
  {
    "objectID": "models.html#leaderboards",
    "href": "models.html#leaderboards",
    "title": "Models",
    "section": "",
    "text": "ChatBot Arena. Elo rating for models (public and private)\nLLM Collection. Collection of many LLMs in one place - check out other sections as well on prompt engineering."
  },
  {
    "objectID": "models.html#open-zoo",
    "href": "models.html#open-zoo",
    "title": "Models",
    "section": "",
    "text": "Models with Permissive Licenses\n\nOpen LLaMA \nmodels | paper | dataset | code | blog |\nAn Open Source reproduction of LLaMA for permissive commerical uses.\n\n\nFalcon \nmodels | paper| dataset | Code | Blog |\nNot based on LLaMA. Can use for commerical purposes. Now finetune.\n\n\nGPT-Neo-X \nmodels | paper | dataset | code | blog |\nMany in this small family at Pythia. Based on The Pile datset.\n\n\nGPT4All, GPT4ALL-LoRA \nmodels | paper | dataset | Code | Blog |\n\n\n\nOpen Assistant \nmodels | paper | dataset| Code | Blog |\n\n\n\nDolly 2 \nmodels | paper | dataset| Code | Blog |\nBased on GPT-Neo-X.\n\n\nBLOOM \nmodels | paper | dataset| Code | Blog |\n\n\n\nCerebras \nmodels | paper | dataset| Code | Blog |"
  },
  {
    "objectID": "models.html#semi-open-zoo",
    "href": "models.html#semi-open-zoo",
    "title": "Models",
    "section": "",
    "text": "Many in this category are, for most part, are derivates of LLaMA. Since LLaMA model weights were never released officially, the following only publish the weight deltas. Thefore, overall model is not permissive for commerical usages.\n\nGuanaco \nmodels | paper| dataset| Code | Blog |\nFine-tune LLaMA on a single GPU via QLoRA.\n\n\nGorilla \nmodels | paper | dataset| Code | Blog |\nBeats GPT-4 on writing API calls.\n\n\nGoat \nmodels | paper | dataset| Code | Blog |\nBeats GPT-4 on Arithmetic tasks. Based on Alpaca-LoRA.\n\n\nStableVicuna \nmodels | paper | dataset| Code | Blog |\n\n\n\nVicuna \nmodels | paper | dataset| Code | Blog |\n\n\n\nKaola \nmodels | paper | dataset| Code | Blog |\nIntroduces EasyLM for code structuring and training in JAX.\n\n\nAlpaca \nmodels | Paper | dataset| code | Blog |\nAlpaca-LoRA can run Raspberry Pi\n\n\nLLaMA \nmodels | Paper | dataset| Blog |"
  }
]