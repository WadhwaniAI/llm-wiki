# Chronicles
> perspectives as they occur


> **Evaluating and uncovering open LLMs ** \
_Nathan Lambert_\
Paper | Code | [Blog](https://www.interconnects.ai/p/evaluating-open-llms) |  YouTube | Twitter | May 2023 \


> **AI is Eating The World ** \
_The Gradient_\
Paper | Code | [Blog](https://txt.cohere.com/ai-is-eating-the-world/) |  YouTube | [Twitter](https://twitter.com/cohereai/status/1657340071496777728?s=46&t=vB01cDnjOMGR22XIeuiGeA) | May 2023 \
Gives a view into the architecture and building blocks needed.

> **Voyager: LLMs play games better than RL ** \
_The Gradient_\
Paper | Code | [Blog](https://voyager.minedojo.org/) |  YouTube | Twitter | May 2023 \

> **Modern AI is Domestification ** \
_The Gradient_\
Paper | Code | [Blog](https://thegradient.pub/ai-is-domestification/?utm_source=substack&utm_medium=email) |  YouTube | Twitter | May 2023 \
What is your moat. Is your prior (on data) same as the LLMs - do prompting. If not fine-tune.
You have no compute. Go for PEFT. If not fine tune. SFT on Imitation data might lead to false superriority as this [Paper](https://arxiv.org/abs/2304.15004) reported. Figure out your priors first. Strategy follows.



