# Hot
> from the press

## June

>**SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression ** \
[Paper](https://arxiv.org/abs/2306.03078) | Dataset | [Tweet](https://twitter.com/Tim_Dettmers/status/1666076553665744896?s=20) | 6-Jun 2023 \
LLMs can fit on your phones now.


>**Deductive Verification of Chain-of-Thought Reasoning ** \
[Paper](https://arxiv.org/abs/2306.03872) | Dataset | [Tweet](https://twitter.com/_akhaliq/status/1666252848781225987?s=20) | 6-Jun 2023 \
CoT: Intermediate steps can also hallucinations. So, apply logic checks.


>**ScoNe: Benchmarking Negation Reasoning in Language Models With Fine-Tuning and In-Context Learning ** \
[Paper](https://arxiv.org/abs/2305.19426) | Dataset | [Tweet](https://twitter.com/ChrisGPotts/status/1666201393697558529?s=20) | 6-Jun 2023 \
Can LLMs handle negation. Easy, not!


>**ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory ** \
[Paper](https://arxiv.org/abs/2306.03901) | Dataset | [Tweet](https://twitter.com/omarsar0/status/1666254609524961282?s=20) | 6-Jun 2023 \
Have LLMs access symbolic memory for complex task planning and execution. 


>**Enabling Large Language Models to Generate Text with Citations ** \
[Paper](https://arxiv.org/abs/2305.14627) | Dataset | [Tweet](https://twitter.com/gaotianyu1350/status/1661186546907639808?s=20) | 5-Jun 2023 \
Asking an LLM to cite seems to be a way of reducing hallucinations, and also allows verification. 

>**Orca: Progressive Learning from Complex Explanation Traces of GPT-4 ** \
[Paper](https://arxiv.org/abs/2306.02707) | Dataset | [Tweet](https://twitter.com/johnjnay/status/1665906453587034112?s=20) | 5-Jun 2023 \
Teach smaller LLM via a bigger LLMs. But watchful of pitfalls of imitation.


>**LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion ** \
[Paper](https://arxiv.org/abs/2306.02561) | Dataset | [Tweet](https://twitter.com/_akhaliq/status/1665887472335695873?s=20) | 5-Jun 2023 \
Another co-op LLM idea.


>**Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback ** \
[Paper](https://arxiv.org/abs/2305.14975) | Dataset | [Tweet](https://twitter.com/topofmlsafety/status/1665732960388100096?s=20) | 5-Jun 2023 \
Calibration in NLP is hard! Now, just ask the LLM to self-report.


>**ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing ** \
[Paper](https://arxiv.org/abs/2306.00622) | Dataset | [Tweet](https://twitter.com/jd92wang/status/1665596397788319744?s=20) | 5-Jun 2023 \
Use LLMs as reviewers:


>**Reimagining Retrieval Augmented Language Models for Answering Queries ** \
[Paper](https://arxiv.org/abs/2306.01061) | Dataset | [Tweet](https://twitter.com/_akhaliq/status/1665554751520485376?s=20) | 2-Jun 2023 \
Semi-parametric architectures can be enhanced with views, a query analyzer/planner, and provenance to make a significantly more powerful system for question answering in terms of accuracy and efficiency, and potentially for other NLP tasks

>**The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only ** \
[Paper](https://arxiv.org/abs/2306.01116) | [Dataset](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) | [Tweet](https://twitter.com/_akhaliq/status/1665555194128609281?s=20) | 2-Jun 2023 \
Publicly release an extract of 600 billion tokens from our RefinedWeb dataset, and 1.3/7.5B parameters language models trained on it. This clean dataset was considered important for Falcon's performance.


>**Decision-Oriented Dialogue for Human-AI Collaboration ** \
[Paper](https://arxiv.org/abs/2305.20076) | [Code](https://github.com/jlin816/dialop) | [Tweet](https://twitter.com/realJessyLin/status/1664410190719111168?s=20) | 2-Jun 2023 \
agents + humans collab to solve hard everyday problems


>**Blockwise Parallel Transformer for Long Context Large Models ** \
[Paper](https://arxiv.org/abs/2305.19370) | Dataset | [Tweet](https://twitter.com/haoliuhl/status/1664396377252667393?s=20) | 2-Jun 2023 \
Train 7B models with over 130K or 13B models with over 64K context windows on just 8 A100 GPUs!

>**Birth of a Transformer: A Memory Viewpoint ** \
[Paper](https://arxiv.org/abs/2306.00802) | Dataset | Blog | 2-Jun 2023 \
Transfomers form backbones in LLMs. How do they work? Thinking fast and slow.
Training data -> Associative Memory. In-context learning ->  Induction Heads.

> **LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model **\
[Paper](https://arxiv.org/abs/2304.15010) | [Code](https://github.com/ZrrSkywalker/LLaMA-Adapter/tree/main/imagebind_LLM) | [Tweet](https://twitter.com/lupantech/status/1664316926003396608?s=20)| 1-Jun 2023 \
Via this Adaptor, LLM can perform open-ended multi-modal instructions by merely introducing 14M parameters over LLaMA. 


>**Editing Commonsense Knowledge in GPT ** \
[Paper](https://arxiv.org/abs/2305.14956) | Dataset | Blog | 2-Jun 2023 \
Outperfm fine-tuning by model-editing based on feedback data.


>**Improving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model ** \
[Paper](https://arxiv.org/abs/2305.17116) | Dataset | Blog | 2-June 2023 \
Applications of RAG (Retrieval Augmented Generation) and evaluation against GPT-4. It is a good pattern to apply!

>**SQL-PaLM: Improved Large Language ModelAdaptation for Text-to-SQL ** \
[Paper](https://arxiv.org/abs/2306.00739) | Dataset | Blog | 2-May 2023 \
A tailed model to inteact with DBs. Another important LLM pattern.

## May

>**Letâ€™s Verify Step by Step ** \
[Paper](https://cdn.openai.com/improving-mathematical-reasoning-with-process-supervision/Lets_Verify_Step_by_Step.pdf) | [Dataset](https://github.com/openai/prm800k) | [Blog](https://openai.com/research/improving-mathematical-reasoning-with-process-supervision) | 31-May 2023 \
to solve math problems with CoT


>**Likelihood-Based Diffusion Language Models ** \
[Paper](https://t.co/Bpa6dpxcrQ) | Code | [Tweet](https://twitter.com/__ishaan/status/1663992335464538117?s=20) |1-June 2023 \
Diffusion comes to LLMs.

>**Tree of Thoughts: Deliberate Problem Solving with Large Language Models **\
[Paper](https://arxiv.org/abs/2305.10601) | Code | Blog | May 2023 \
Generalzie CoT and solve harder problems


>**Training Trajectories of Language Models Across Scales ** \
[Paper](https://arxiv.org/abs/2212.09803) | [Code](https://github.com/xiamengzhou/training_trajectory_analysis) | [Tweet](https://twitter.com/xiamengzhou/status/1663742017568899073?s=20) |31-May 2023 \
How do LLMs learn during training? Touches aspects like double-descent, hallucination.\
Perplexity is a strong predictor of in-context learning performance.


>**PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning ** \
[Paper](https://arxiv.org/abs/2305.19472) | Code | [Tweet](https://twitter.com/johnjnay/status/1664079539905994752?s=20) |1-June 2023 \
Disticll symbolic knowledge. Small LLMs beat large LLMs in counter-factual planning.

>**Deliberate then Generate: Enhanced Prompting Framework for Text Generation ** \
[Paper](https://arxiv.org/abs/2305.19835) | Code | [Tweet](https://twitter.com/omarsar0/status/1664079620797353984?s=20) |1-June 2023 \
Another technique in prompt-engineering


>**Direct Preference Optimization: Your Language Model is Secretly a Reward Model ** \
[Paper](https://arxiv.org/abs/2305.18290) | Code | [Tweet](https://twitter.com/archit_sharma97/status/1663595372269408261?s=20) | 30-May 2023 \
Dont need RL in training LLMs. Leads to quicker computation.

>**Improving Factuality and Reasoning in Language Models through Multiagent Debate ** \
[Paper](https://arxiv.org/abs/2305.14325) | Code | Blog | 30-May 2023 \
LLM Co-Pilots working with each other.


>**Model Dementia: Generated Data Makes Models Forget ** \
[Paper](https://arxiv.org/abs/2305.17493) | Code | Blog | 30-May 2023 \
What if all data we find on the internet is LLM generated?


>**BigTrans: Augmenting Large Language Models with Multilingual Translation Capability over 100 Languages ** \
[Paper](https://arxiv.org/abs/2305.18098) | [Code](https://github.com/ZNLP/BigTrans) | Blog | 30-May 2023 \
Enhance the language translaion capablities of LLMs

>**BiomedGPT: BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks ** \
[Paper](https://arxiv.org/abs/2305.17100) | [Code](https://github.com/taokz/BiomedGPT) | Blog | 29-May 2023 \
Object Detection, Captioning, NLI, VQA, Classification, Masked LMs