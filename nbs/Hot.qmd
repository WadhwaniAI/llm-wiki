# Hot
> from the press

>**Improving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model ** \
[Paper](https://arxiv.org/abs/2305.17116) | Dataset | Blog | 2-May 2023 \
Applications of RAG (Retrieval Augmented Generation) and evaluation against GPT-4. It is a good pattern to apply!

>**SQL-PaLM: Improved Large Language ModelAdaptation for Text-to-SQL ** \
[Paper](https://arxiv.org/abs/2306.00739) | Dataset | Blog | 2-May 2023 \
A tailed model to inteact with DBs. Another important LLM pattern.


>**Letâ€™s Verify Step by Step ** \
[Paper](https://cdn.openai.com/improving-mathematical-reasoning-with-process-supervision/Lets_Verify_Step_by_Step.pdf) | [Dataset](https://github.com/openai/prm800k) | [Blog](https://openai.com/research/improving-mathematical-reasoning-with-process-supervision) | 31-May 2023 \
to solve math problems with CoT


>**Likelihood-Based Diffusion Language Models ** \
[Paper](https://t.co/Bpa6dpxcrQ) | Code | [Tweet](https://twitter.com/__ishaan/status/1663992335464538117?s=20) |1-June 2023 \
Diffusion comes to LLMs.

>**Tree of Thoughts: Deliberate Problem Solving with Large Language Models **\
[Paper](https://arxiv.org/abs/2305.10601) | Code | Blog | May 2023 \
Generalzie CoT and solve harder problems


>**Training Trajectories of Language Models Across Scales ** \
[Paper](https://arxiv.org/abs/2212.09803) | [Code](https://github.com/xiamengzhou/training_trajectory_analysis) | [Tweet](https://twitter.com/xiamengzhou/status/1663742017568899073?s=20) |31-May 2023 \
How do LLMs learn during training? Touches aspects like double-descent, hallucination.\
Perplexity is a strong predictor of in-context learning performance.


>**PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning ** \
[Paper](https://arxiv.org/abs/2305.19472) | Code | [Tweet](https://twitter.com/johnjnay/status/1664079539905994752?s=20) |1-June 2023 \
Disticll symbolic knowledge. Small LLMs beat large LLMs in counter-factual planning.

>**Deliberate then Generate: Enhanced Prompting Framework for Text Generation ** \
[Paper](https://arxiv.org/abs/2305.19835) | Code | [Tweet](https://twitter.com/omarsar0/status/1664079620797353984?s=20) |1-June 2023 \
Another technique in prompt-engineering


>**Direct Preference Optimization: Your Language Model is Secretly a Reward Model ** \
[Paper](https://arxiv.org/abs/2305.18290) | Code | [Tweet](https://twitter.com/archit_sharma97/status/1663595372269408261?s=20) | 30-May 2023 \
Dont need RL in training LLMs. Leads to quicker computation.

>**Improving Factuality and Reasoning in Language Models through Multiagent Debate ** \
[Paper](https://arxiv.org/abs/2305.14325) | Code | Blog | 30-May 2023 \
LLM Co-Pilots working with each other.


>**Model Dementia: Generated Data Makes Models Forget ** \
[Paper](https://arxiv.org/abs/2305.17493) | Code | Blog | 30-May 2023 \
What if all data we find on the internet is LLM generated?


>**BigTrans: Augmenting Large Language Models with Multilingual Translation Capability over 100 Languages ** \
[Paper](https://arxiv.org/abs/2305.18098) | [Code](https://github.com/ZNLP/BigTrans) | Blog | 30-May 2023 \
Enhance the language translaion capablities of LLMs

>**BiomedGPT: BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks ** \
[Paper](https://arxiv.org/abs/2305.17100) | [Code](https://github.com/taokz/BiomedGPT) | Blog | 29-May 2023 \
Object Detection, Captioning, NLI, VQA, Classification, Masked LMs