# Datasets
> THE moat!



> **Datasets  Hub  ** \
[datasets](https://github.com/Zjh-819/LLMDataHub) |\
Tagged by Task like CoT, SFT, PT,...



## Datasets

>**MultiLegalPile: A 689GB Multilingual Legal Corpus ** \
[Paper](https://arxiv.org/abs/2306.02069) | [Dataset](https://huggingface.co/datasets/joelito/Multi_Legal_Pile) | [Code](https://github.com/JoelNiklaus/LegalDatasets) | [Tweet](https://twitter.com/joelniklaus/status/1665983164764897281?s=20) | 2-Jun 2023 \
Legal Data - LegalGPT :P

> **Falcon Data  ** \
[models](https://huggingface.co/tiiuae/falcon-40b-instruct) | [dataset](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) | Code | [Tweet](https://twitter.com/qlhoest/status/1663548217022357509?s=20) | \
5k+parquet files, 960m+samples, 500-600 tokens

>**The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only ** \
[Paper](https://arxiv.org/abs/2306.01116) | [Dataset](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) | [Tweet](https://twitter.com/_akhaliq/status/1665555194128609281?s=20) | 2-Jun 2023 \
Publicly release an extract of 600 billion tokens from our RefinedWeb dataset, and 1.3/7.5B parameters language models trained on it. This clean dataset was considered important for Falcon's performance.

> **The Pile  ** \
[paper](https://arxiv.org/abs/2101.00027) | [dataset](https://pile.eleuther.ai/) | Code | Blog | \
An 825GB set consisting of 22 smaller, high quality datasets to train LLMs.\ 
It is THE dataset from [eleuther](https://www.eleuther.ai/)

> **used in Alpaca  ** \
[paper](https://arxiv.org/abs/2302.13971v1) | [dataset](https://github.com/tatsu-lab/stanford_alpaca#data-release) | Code | [Blog](https://crfm.stanford.edu/2023/03/13/alpaca.html) | \
52k instruction following data used in fine-tuning Alpaca model. This data was produced using GPT-3. So beware of the false superiority of the imitattion models.

> **used in gpt4all, gpt4all-lora  ** \
[gpt4all](https://static.nomic.ai/gpt4all/2023_GPT4All-J_Technical_Report_2.pdf) | [dataset](https://huggingface.co/datasets/nomic-ai/gpt4all-j-prompt-generations) | [Code](https://github.com/nomic-ai/gpt4all) | [Blog](https://gpt4all.io/index.html) | \
Accessible from HuggingFace datasets library.

> **The OIG dataset  ** \
paper | [dataset](https://laion.ai/blog/oig-dataset/) | Code | Blog | \
43M Instruction dataset to convert a pre-trained LLM to follow instructions.

> **from Anthropic  ** \
paper | [dataset](https://huggingface.co/datasets/Anthropic/hh-rlhf/) | Code | Blog | \
Human preference data to reduce model toxicity. Accessible from HuggingFace datasets library. 

> **from openAI  ** \
[paper](https://arxiv.org/abs/2112.09332) | [dataset](https://huggingface.co/datasets/openai/webgpt_comparisons) | Code | Blog | \
A long-form QA to align model with humans. Accessible from HuggingFace datasets library.

> **from openAI  ** \
[paper](https://arxiv.org/abs/2009.01325) | [dataset](https://huggingface.co/datasets/openai/summarize_from_feedback) | Code | Blog | \
train a reward model, which helps an LLM align with Humans. Accessible from HuggingFace datasets library.

> **Stanford Human Preferences Dataset ** \
[paper](https://arxiv.org/abs/2009.01325) | [dataset](https://huggingface.co/datasets/stanfordnlp/SHP) | Code | Blog | \
Models like Stable Vicuna are trained on this.

## Synthetic Data Creation Tools
> **Dynosuar: Data Curation Framework ** \
paper | dataset | Code | [Blog](https://dynosaur-it.github.io/) \
Use other LLMs to generate data.

> **Tool LLaMA ** \
paper | dataset | [Code](https://github.com/OpenBMB/ToolBench) | [Tweet](https://twitter.com/TsingYoga/status/1662843257796333568?s=20) \
Create high quality data to train LLMs

> **TxtInstruct  ** \
[doc]() | [repo](https://github.com/neuml/txtinstruct) |\
Datasets and Models for Instruction Tuning. See this [example](https://colab.research.google.com/github/neuml/txtinstruct/blob/master/examples/01_Introducing_txtinstruct.ipynb).

