# The How
> Under the hood, why & how LLMs work?


> **Language Models are Few-Shot Learners **\
[Paper](https://arxiv.org/abs/2005.14165) | Code | Blog | May 2020 \
Ditch the paradigm of pre-train, fine-tune. Towards One model, many tasks.

> **What Can Transformers Learn In-Context? A Case Study of Simple Function Classes **\
[Paper](https://arxiv.org/abs/2208.01066) | Code | Blog | Aug 2022 \
Using Linear models as a probing device - shows what ICL does?


> **What learning algorithm is in-context learning? Investigations with linear models **\
[Paper](https://arxiv.org/abs/2211.15661) | Code | Blog | Nov 2022 \
in-context learning recovers standard algorithms in linear model setup. 

> **Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers **\
[Paper](https://arxiv.org/abs/2212.10559v2) | Code | Blog | Dec 2022 \
Uses linear models as a lens to understand what in-context learnign does? It is a meta gradiant optimizer.


> **Scaling Data-Constrained Language Models **\
[Paper](https://arxiv.org/abs/2305.16264) | [Code](https://github.com/huggingface/datablations) | Blog | May 2023 \
Scale data and model size. How big is big? Can data be reused and selectively reused? Yes. Has someuseful datasets! 

> **LIMA: Less Is More for Alignment **\
[Paper](https://arxiv.org/abs/2305.11206) | Code | Blog | May 2023 \
A relief. Low volume but high quality instruction data is good. No RLHF required!!!


> **How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection **\
[Paper](https://arxiv.org/abs/2301.07597) | Code | Blog | date \
Easy jobs will be eiliminated. Higher order skills needed to survive. 

> **Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness **\
[Paper](https://arxiv.org/abs/2304.11633) | Code | Blog | May 2023 \
Faithful and Explainable, but overconfident. GPTs work well in Open IE tasks but on Closed IE tasks, narrow, purpose models can be better.