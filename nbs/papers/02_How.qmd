# The How & Why
> Under the hood, why & how LLMs work?

## Perspectives
> **Language Models are Few-Shot Learners **\
[Paper](https://arxiv.org/abs/2005.14165) | Code | Blog | May 2020 \
Ditch the paradigm of pre-train, fine-tune. Towards One model, many tasks.

## Probes

>**Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions ** \
[Paper](https://arxiv.org/abs/2305.15083) | Dataset | Blog | Jun 2023 \
Even when not explicitly trained language translation tasks, LLMs exhibit good zero shot ability.


>**Birth of a Transformer: A Memory Viewpoint ** \
[Paper](https://arxiv.org/abs/2306.00802) | Dataset | Blog | 2-Jun 2023 \
Transfomers form backbones in LLMs. How do they work? Thinking fast and slow.
Training data -> Associative Memory. In-context learning ->  Induction Heads.

>**Intriguing Properties of Quantization at Scale ** \
[Paper](https://arxiv.org/abs/2305.19268) | Code | [Tweet](https://twitter.com/sarahookr/status/1663925847890497539?s=20) |31-May 2023 \
Are quantization cliffs in performance solely a factor of scale?\ Apparently not. Quantize models ranging in size from 410M to 52B with minimal degradation in performance

>**Training Trajectories of Language Models Across Scales ** \
[Paper](https://arxiv.org/abs/2212.09803) | [Code](https://github.com/xiamengzhou/training_trajectory_analysis) | [Tweet](https://twitter.com/xiamengzhou/status/1663742017568899073?s=20) |31-May 2023 \
How do LLMs learn during training? Touches aspects like double-descent, hallucination.\
Perplexity is a strong predictor of in-context learning performance.

## In-Context Learning

> **What Can Transformers Learn In-Context? A Case Study of Simple Function Classes **\
[Paper](https://arxiv.org/abs/2208.01066) | Code | Blog | Aug 2022 \
Using Linear models as a probing device - shows what ICL does?

> **What learning algorithm is in-context learning? Investigations with linear models **\
[Paper](https://arxiv.org/abs/2211.15661) | Code | Blog | Nov 2022 \
in-context learning recovers standard algorithms in linear model setup. 

> **Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers **\
[Paper](https://arxiv.org/abs/2212.10559v2) | Code | Blog | Dec 2022 \
Uses linear models as a lens to understand what in-context learnign does? It is a meta gradiant optimizer.

## Scaling Laws

> **Scaling Data-Constrained Language Models **\
[Paper](https://arxiv.org/abs/2305.16264) | [Code](https://github.com/huggingface/datablations) | Blog | May 2023 \
Scale data and model size. How big is big? Can data be reused and selectively reused? Yes. Has someuseful datasets! 

> **LIMA: Less Is More for Alignment **\
[Paper](https://arxiv.org/abs/2305.11206) | Code | Blog | May 2023 \
A relief. Low volume but high quality instruction data is good. No RLHF required!!!

## Evaluation
> **How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection **\
[Paper](https://arxiv.org/abs/2301.07597) | Code | Blog | date \
Easy jobs will be eiliminated. Higher order skills needed to survive. 

> **Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness **\
[Paper](https://arxiv.org/abs/2304.11633) | Code | Blog | May 2023 \
Faithful and Explainable, but overconfident. GPTs works well in Open IE tasks but on Closed IE tasks, narrow, purpose models can be better.